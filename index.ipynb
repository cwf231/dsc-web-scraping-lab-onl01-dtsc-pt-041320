{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you've seen a more extensive example of developing a web scraping script, it's time to further practice and formalize that knowledge by writing functions to parse specific pieces of information from the web page and then synthesizing these into a larger loop that will iterate over successive web pages in order to build a complete dataset.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Navigate HTML documents using Beautiful Soup's children and sibling relations\n",
    "* Select specific elements from HTML using Beautiful Soup\n",
    "* Use regular expressions to extract items with a certain pattern within Beautiful Soup\n",
    "* Determine the pagination scheme of a website and scrape multiple pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Overview\n",
    "\n",
    "This lab will build upon the previous lesson. In the end, you'll look to write a script that will iterate over all of the pages for the demo site and extract the title, price, star rating and availability of each book listed. Building up to that, you'll formalize the concepts from the lesson by writing functions that will extract a list of each of these features for each web page. You'll then combine these functions into the full script which will look something like this:  \n",
    "\n",
    "```python\n",
    "df = pd.DataFrame()\n",
    "for i in range(2,51):\n",
    "    url = \"http://books.toscrape.com/catalogue/page-{}.html\".format(i)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    new_titles = retrieve_titles(soup)\n",
    "    new_star_ratings = retrieve_ratings(soup)\n",
    "    new_prices = retrieve_prices(soup)\n",
    "    new_avails = retrieve_avails(soup)\n",
    "    ...\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Titles\n",
    "\n",
    "To start, write a function that extracts the titles of the books on a given page. The input for the function should be the `soup` for the HTML of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Ratings\n",
    "\n",
    "Next, write a similar function to retrieve the star ratings on a given page. Again, the function should take in the `soup` from the given HTML page and return a list of the star ratings for the books. These star ratings should be formatted as integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Prices\n",
    "\n",
    "Now write a function to retrieve the prices on a given page. The function should take in the `soup` from the given page and return a list of prices formatted as floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Availability\n",
    "\n",
    "Write a function to retrieve whether each book is available or not. The function should take in the `soup` from a given html page and return a list of the availability for each book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Script to Retrieve All the Books From All 50 Pages\n",
    "\n",
    "Finally, write a script to retrieve all of the information from all 50 pages of the books.toscrape.com website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level-Up: Write a new version of the script you just wrote. \n",
    "\n",
    "If you used URL hacking to generate each successive page URL, instead write a function that retrieves the link from the `\"next\"` button at the bottom of the page. Conversely, if you already used this approach above, use URL-hacking (arguably the easier of the two methods in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "\n",
    "def get_book_container(soup):\n",
    "    warning = soup.find('div', class_='alert alert-warning')\n",
    "    return warning.nextSibling.nextSibling\n",
    "\n",
    "\n",
    "def get_titles(book_container):\n",
    "    return [row.find('a')['title'] for row \n",
    "            in book_container.findAll('h3')]\n",
    "\n",
    "\n",
    "def get_ratings(book_container):\n",
    "    regex = re.compile(\"^star-rating\")\n",
    "    ratings = [each['class'][-1] for each \n",
    "               in book_container.findAll('p', {'class': regex})]\n",
    "    \n",
    "    rating_dct = {\n",
    "        'One': 1, 'Two': 2, 'Three':3, 'Four': 4, 'Five':5\n",
    "    }\n",
    "    \n",
    "    return [rating_dct[rating] for rating in ratings]\n",
    "\n",
    "\n",
    "def get_prices(book_container):\n",
    "    return [float(each.text[1:]) for each \n",
    "            in book_container.findAll('p', class_='price_color')]\n",
    "\n",
    "\n",
    "def get_avail(book_container):\n",
    "    return [each.text.strip() for each \n",
    "            in book_container.findAll('p', class_='instock availability')]\n",
    "\n",
    "\n",
    "def get_images(book_container, overall_counter):\n",
    "    counter = 1\n",
    "    lst = []\n",
    "    for each in book_container.findAll('img'):\n",
    "        url = 'http://books.toscrape.com/'\n",
    "        url_ext = each['src']\n",
    "        full_url = url + url_ext\n",
    "\n",
    "        r = requests.get(full_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            f = open(f'images/book{counter}-{overall_counter}.jpg', 'wb')\n",
    "\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "        lst.append(f'<img src=\"images/book{counter}-{overall_counter}.jpg\"/>')\n",
    "        counter += 1\n",
    "    return lst\n",
    "\n",
    "    \n",
    "def create_df():\n",
    "    print('Working...')\n",
    "    counter = 0\n",
    "    new_tag = ''\n",
    "\n",
    "    titles = []\n",
    "    ratings = []\n",
    "    prices = []\n",
    "    avail = []\n",
    "    imgs = []\n",
    "    \n",
    "    while True:\n",
    "        url = 'http://books.toscrape.com/'\n",
    "        url += new_tag\n",
    "        \n",
    "        counter += 1\n",
    "        print(f'Page {counter}')\n",
    "\n",
    "        req_get = requests.get(url)\n",
    "        soup_curr = BeautifulSoup(req_get.content, 'html')\n",
    "        \n",
    "        book_container = get_book_container(soup_curr)\n",
    "        \n",
    "        titles += get_titles(book_container)\n",
    "        ratings += get_ratings(book_container)\n",
    "        prices += get_prices(book_container)\n",
    "        avail += get_avail(book_container)\n",
    "        imgs += get_images(book_container, counter)\n",
    "        \n",
    "        try:\n",
    "            new_tag = soup_curr.find('li', class_='next').find('a')['href']\n",
    "        except AttributeError:\n",
    "            print(f'Completed with {counter} pages.')\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                [titles, ratings, prices, avail, imgs]\n",
    "            ).transpose()\n",
    "    \n",
    "            df.columns = ['title', 'rating', 'price_pounds', 'avail', 'img']\n",
    "        \n",
    "            print('DataFrame Shape:', df.shape)\n",
    "            return df\n",
    "        \n",
    "        if 'catalogue' not in new_tag:\n",
    "            new_tag = 'catalogue/' + new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working...\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Page 17\n",
      "Page 18\n",
      "Page 19\n",
      "Page 20\n",
      "Page 21\n",
      "Page 22\n",
      "Page 23\n",
      "Page 24\n",
      "Page 25\n",
      "Page 26\n",
      "Page 27\n",
      "Page 28\n",
      "Page 29\n",
      "Page 30\n",
      "Page 31\n",
      "Page 32\n",
      "Page 33\n",
      "Page 34\n",
      "Page 35\n",
      "Page 36\n",
      "Page 37\n",
      "Page 38\n",
      "Page 39\n",
      "Page 40\n",
      "Page 41\n",
      "Page 42\n",
      "Page 43\n",
      "Page 44\n",
      "Page 45\n",
      "Page 46\n",
      "Page 47\n",
      "Page 48\n",
      "Page 49\n",
      "Page 50\n",
      "Completed with 50 pages.\n",
      "DataFrame Shape: (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "df = create_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         1000 non-null   object\n",
      " 1   rating        1000 non-null   object\n",
      " 2   price_pounds  1000 non-null   object\n",
      " 3   avail         1000 non-null   object\n",
      " 4   img           1000 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 39.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>price_pounds</th>\n",
       "      <th>avail</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>3</td>\n",
       "      <td>51.77</td>\n",
       "      <td>In stock</td>\n",
       "      <td><img src=\"images/book1-1.jpg\"/></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>1</td>\n",
       "      <td>53.74</td>\n",
       "      <td>In stock</td>\n",
       "      <td><img src=\"images/book2-1.jpg\"/></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>1</td>\n",
       "      <td>50.1</td>\n",
       "      <td>In stock</td>\n",
       "      <td><img src=\"images/book3-1.jpg\"/></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>4</td>\n",
       "      <td>47.82</td>\n",
       "      <td>In stock</td>\n",
       "      <td><img src=\"images/book4-1.jpg\"/></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>5</td>\n",
       "      <td>54.23</td>\n",
       "      <td>In stock</td>\n",
       "      <td><img src=\"images/book5-1.jpg\"/></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(df.head().to_html(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Images\n",
    "### Scraping and adding to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://books.toscrape.com/'\n",
    "# html_page = requests.get(url)\n",
    "# soup = BeautifulSoup(html_page.content, 'html')\n",
    "# warning = soup.find('div', class_='alert alert-warning')\n",
    "# book_container = warning.nextSibling.nextSibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_img = book_container.findAll('img')[0]\n",
    "# ex_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 1\n",
    "# for each in book_container.findAll('img'):\n",
    "#     url_ext = each['src']\n",
    "#     full_url = url + url_ext\n",
    "\n",
    "#     r = requests.get(full_url, stream=True)\n",
    "#     if r.status_code == 200:\n",
    "#         f = open(f'images/book{counter}.jpg', 'wb')\n",
    "        \n",
    "#         r.raw.decode_content = True\n",
    "#         shutil.copyfileobj(r.raw, f)\n",
    "        \n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = mpimg.imread('images/book1.jpg')\n",
    "# imgplot = plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display img in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row1 = [ex_img['alt'], '<img src=\"images/book1.jpg\"/>']\n",
    "# row1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.DataFrame(row1).transpose()\n",
    "# test_df.columns = ['title', 'img']\n",
    "# HTML(test_df.to_html(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
